{"title": "TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring", "abstract": "Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for nontarget prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.", "text": [{"id": 0, "string": "Introduction Automated essay scoring (AES) utilizes natural language processing and machine learning techniques to automatically rate essays written for a target prompt (Dikli, 2006) ."}, {"id": 1, "string": "Currently, the AES systems have been widely used in large-scale English writing tests, e.g."}, {"id": 2, "string": "Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006) ."}, {"id": 3, "string": "Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999) ."}, {"id": 4, "string": "While the established models are effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; , we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible."}, {"id": 5, "string": "For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data."}, {"id": 6, "string": "Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays written for the target prompt, as well as the rated essays for several non-target prompts."}, {"id": 7, "string": "We argue that it is not straightforward, if possible, to apply the established promptdependent AES methods for the mentioned prompt-independent scenario."}, {"id": 8, "string": "On one hand, essays for different prompts may differ a lot in the uses of vocabulary, the structure, and the grammatic characteristics; on the other hand, however, established prompt-dependent AES models are designed to learn from these prompt-specific features, including the on/off-topic degree, the tfidf weights of topical terms (Attali and Burstein, 2006; Dikli, 2006) , and the n-gram features extracted from word semantic embeddings (Dong and Zhang, 2016; Alikaniotis et al., 2016) ."}, {"id": 9, "string": "Consequently, the prompt-dependent models can hardly learn generalized rules from rated essays for nontarget prompts, and are not suitable for the promptindependent AES."}, {"id": 10, "string": "Being aware of this difficulty, to this end, a twostage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem."}, {"id": 11, "string": "In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage, a shallow model is trained on a number of rated essays for several non-target prompts; given a target prompt and a set of essays to rate, the trained model is employed to generate pseudo training data by selecting essays with the extreme quality."}, {"id": 12, "string": "At the second stage, a novel end-to-end hybrid deep neural network learns prompt-dependent features from these selected training data, by considering semantic, part-of-speech, and syntactic features."}, {"id": 13, "string": "The contributions in this paper are threefold: 1) a two-stage learning framework is proposed to bridge the gap between the target and non-target prompts, by only consuming rated essays for nontarget prompts as training data; 2) a novel deep model is proposed to learn from pseudo labels by considering semantic, part-of-speech, and syntactic features; and most importantly, 3) to the best of our knowledge, the proposed TDNN is actually the first approach dedicated to addressing the prompt-independent AES."}, {"id": 14, "string": "Evaluation on the standard ASAP dataset demonstrates the effectiveness of the proposed method."}, {"id": 15, "string": "The rest of this paper is organized as follows."}, {"id": 16, "string": "In Section 2, we describe our novel TDNN model, including the two-stage framework and the proposed deep model."}, {"id": 17, "string": "Following that, we describe the setup of our empirical study in Section 3, thereafter present the results and provide analyzes in Section 4."}, {"id": 18, "string": "Section 5 recaps existing literature and put our work in context, before drawing final conclusions in Section 6."}, {"id": 19, "string": "Two-stage Deep Neural Network for AES In this section, the proposed two-stage deep neural network (TDNN) for prompt-independent AES is described."}, {"id": 20, "string": "To accurately rate an essay, on one hand, we need to consider its pertinence to the given prompt; on the other hand, the organization, the analyzes, as well as the uses of the vocabulary are all crucial for the assessment."}, {"id": 21, "string": "Henceforth, both prompt-dependent and -independent factors should be considered, but the latter ones actually do not require prompt-dependent training data."}, {"id": 22, "string": "Accordingly, in the proposed framework, a supervised ranking model is first trained to learn from prompt-independent data, hoping to roughly assess essays without considering the prompt; subsequently, given the test dataset, namely, a set of essays for a target prompt, a subset of essays are selected as positive and negative training data based on the prediction of the trained model from the first stage; ultimately, a novel deep model is proposed to learn both prompt-dependent and -independent factors on this selected subset."}, {"id": 23, "string": "As indicated in Figure 1 , the proposed framework includes two stages."}, {"id": 24, "string": "Prompt-independent stage."}, {"id": 25, "string": "Only the promptindependent factors are considered to train a shallow model, aiming to recognize the essays with the extreme quality in the test dataset, where the rated essays for non-target prompts are used for training."}, {"id": 26, "string": "Intuitively, one could recognize essays with the highest and the lowest scores correctly by solely examining their quality of writing, e.g., the number of typos, without even understanding them, and the prompt-independent features such as the number of grammatic and spelling errors should be sufficient to fulfill this screening procedure."}, {"id": 27, "string": "Accordingly, a supervised model trained solely on prompt-independent features is employed to identify the essays with the highest and lowest scores in a given set of essays for the target prompt, which are used as the positive and negative training data in the follow-up prompt-dependent learning phase."}, {"id": 28, "string": "Overview Prompt-dependent stage."}, {"id": 29, "string": "Intuitively, most essays are with a quality in between the extremes, requiring a good understanding of their meaning to make an accurate assessment, e.g., whether the examples from the essay are convincing or whether the analyzes are insightful, making the consideration of prompt-dependent features crucial."}, {"id": 30, "string": "To achieve that, a model is trained to learn from the comparison between essays with the highest and lowest scores for the target prompt according to the predictions from the first step."}, {"id": 31, "string": "Akin to the settings in transductive transfer learning (Pan and Yang, 2010), given essays for a particular prompt, quite a few confident essays at two extremes are selected and are used to train another model for a fine-grained content-based prompt-dependent assessment."}, {"id": 32, "string": "To enable this, a powerful deep model is proposed to consider the content of the essays from different perspectives using semantic, part-of-speech (POS) and syntactic network."}, {"id": 33, "string": "After being trained with the selected essays, the deep model is expected to memorize the properties of a good essay in response to the target prompt, thereafter accurately assessing all essays for it."}, {"id": 34, "string": "In Section 2.2, building blocks for the selection of the training data and the proposed deep model are described in details."}, {"id": 35, "string": "Building Blocks Select confident essays as training data."}, {"id": 36, "string": "The identification of the extremes is relatively simple, where a RankSVM (Joachims, 2002) is trained on essays for different non-target prompts, avoiding the risks of over-fitting some particular prompts."}, {"id": 37, "string": "A set of established prompt-independent features are employed, which are listed in Table 2 ."}, {"id": 38, "string": "Given a prompt and a set of essays for evaluation, to begin with, the trained RankSVM is used to assign prediction scores to individual prompt-essay pairs, which are uniformly transformed into a 10point scale."}, {"id": 39, "string": "Thereafter, the essays with predicted scores in [0, 4] and [8, 10] are selected as negative and positive examples respectively, serving as the bad and good templates for training in the next stage."}, {"id": 40, "string": "Intuitively, an essay with a score beyond eight out of a 10-point scale is considered good, while the one receiving less than or equal to four, is considered to be with a poor quality."}, {"id": 41, "string": "A hybrid deep model for fine-grained assessment."}, {"id": 42, "string": "To enable a prompt-dependent assessment, a model is desired to comprehensively capture the ways in which a prompt is described or discussed in an essay."}, {"id": 43, "string": "In this paper, semantic meaning, part-of-speech (POS), and the syntactic taggings of the token sequence from an essay are considered, grasping the quality of an essay for a target prompt."}, {"id": 44, "string": "The model architecture is summarized in Figure 2 ."}, {"id": 45, "string": "Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, denoted as \u2212 \u2192 e sem , hoping to understand what the essay is about; in addition, the part-of-speech information is encoded as a sequence of POS tag-gings, coined as \u2212 \u2192 e pos ; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured via syntactic network, leading to \u2212 \u2192 e synt , where the model learns the organization of the essay."}, {"id": 46, "string": "Akin to (Li et al., 2015) and (Zhou and Xu, 2015) , bi-LSTM is employed as a basic component to encode a sequence."}, {"id": 47, "string": "Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating."}, {"id": 48, "string": "In the following, the architecture of the model is described in details."}, {"id": 49, "string": "-Semantic embedding."}, {"id": 50, "string": "Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016) , semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014) , are employed."}, {"id": 51, "string": "On top of the word embeddings, two bi-LSTM layers are stacked, namely, the essay layer is constructed on top of the sentence layer, ending up with the semantic representation of the whole essay, which is denoted as \u2212 \u2192 e sem in Figure 2 ."}, {"id": 52, "string": "-Part-Of-Speech (POS) embeddings for individual terms are first generated by the Stanford Tagger (Toutanova et al., 2003) , where 36 different POS tags present."}, {"id": 53, "string": "Accordingly, individual words are embedded with 36-dimensional one-hot representation, and is transformed to a 50-dimensional vector through a lookup layer."}, {"id": 54, "string": "After that, two bi-LSTM layers are stacked, leading to \u2212 \u2192 e pos ."}, {"id": 55, "string": "Take Figure 3 for example, given a sentence \"Attention please, here is an example."}, {"id": 56, "string": "\", it is first converted into a POS sequence using the tagger, namely, VB, VBP, RB, VBZ, DT, NN; thereafter it is further mapped to vector space through one-hot embedding and a lookup layer."}, {"id": 57, "string": "-Syntactic embedding aims at encoding an essay in terms of the syntactic relationships among different syntactic components, by encoding an essay recursively."}, {"id": 58, "string": "The Stanford Parser (Socher et al., 2013) is employed to label the syntactic structure of words and phrases in sentences, accounting for 59 different types in total."}, {"id": 59, "string": "Similar to (Tai et al., 2015) , we opt for three stacked bi-LSTM, aiming at encoding individual phrases, sentences, and ultimately the whole essay in sequence."}, {"id": 60, "string": "In particular, according to the hierarchical structure from a parsing tree, the phrase-level bi-LSTM first encodes different phrases by consuming syntactic Figure 2 ) from a lookup table of individual syntactic units in the tree; thereafter, the encoded dense layers in individual sentences are further consumed by a sentence-level bi-LSTM, ending up with sentence-level syntactic representations, which are ultimately combined by the essay-level bi-LSTM, resulting in \u2212 \u2192 e synt ."}, {"id": 61, "string": "For example, the parsed tree for a sentence \"Attention please, here is an example.\""}, {"id": 62, "string": "is displayed in Figure 3 ."}, {"id": 63, "string": "To start with, the sentence is parsed into ((NP VP)(NP VP NP)), and the dense embeddings are fetched from a lookup table for all tokens, namely, NP and VP; thereafter, the phraselevel bi-LSTM encodes (NP VP) and (NP VP N-P) separately, which are further consumed by the sentence-level bi-LSTM."}, {"id": 64, "string": "Afterward, essay-level bi-LSTM further combines the representations of different sentences into \u2212 \u2192 e synt ."}, {"id": 65, "string": "(ROOT (S (S (NP (VB Attention)) (VP (VBP please))) (, ,) (NP (RB here)) (VP (VBZ is) (NP (DT an) (NN example))) (."}, {"id": 66, "string": ".)))"}, {"id": 67, "string": "Figure 3: An example of the context-free phrase structure grammar tree."}, {"id": 68, "string": "-Combination."}, {"id": 69, "string": "A feed-forward network linearly transforms the concatenated representations of an essay from the mentioned three perspectives into a scalar, which is further normalized into [0, 1] with a sigmoid function."}, {"id": 70, "string": "Objective and Training Objective."}, {"id": 71, "string": "Mean square error (MSE) is optimized, which is widely used as a loss function in regression tasks."}, {"id": 72, "string": "Given N pairs of a target prompt p i and an essay e i , MSE measures the average value of square error between the normalized gold standard rating r * (p i , e i ) and the predicted rating r(p i , e i ) assigned by the AES model, as summarized in Equation 1."}, {"id": 73, "string": "1 N N \u2211 i=1 ( r(p i , e i ) \u2212 r * (p i , e i ) ) 2 (1) Optimization."}, {"id": 74, "string": "Adam (Kingma and Ba, 2014) is employed to minimize the loss over the training data."}, {"id": 75, "string": "The initial learning rate \u03b7 is set to 0.01 and the gradient is clipped between [\u221210, 10] during training."}, {"id": 76, "string": "In addition, dropout (Srivastava et al., 2014) is introduced for regularization with a dropout rate of 0.5, and 64 samples are used in each batch with batch normalization (Ioffe and Szegedy, 2015) ."}, {"id": 77, "string": "30% of the training data are reserved for validation."}, {"id": 78, "string": "In addition, early stopping (Yao et al., 2007) is employed according to the validation loss, namely, the training is terminated if no decrease of the loss is observed for ten consecutive epochs."}, {"id": 79, "string": "Once training is finished, Prompt #Essays Avg Length Score Range   1  1783  350  2-12  2  1800  350  1-6  3  1726  150  0-3  4  1772  150  0-3  5  1805  150  0-4  6  1800  150  0-4  7  1569  250  0-30  8  723  650  0-60   Table 1 : Statistics for the ASAP dataset."}, {"id": 80, "string": "akin to , the model with the best quadratic weighted kappa on the validation set is selected."}, {"id": 81, "string": "3 Experimental Setup Dataset."}, {"id": 82, "string": "The Automated Student Assessment Prize (ASAP) dataset has been widely used for AES (Alikaniotis et al., 2016; Chen and He, 2013; , and is also employed as the prime evaluation instrument herein."}, {"id": 83, "string": "In total, AS-AP consists of eight sets of essays, each of which associates to one prompt, and is originally written by students between Grade 7 and Grade 10."}, {"id": 84, "string": "As summarized in Table 1 , essays from different sets differ in their rating criteria, length, as well as the rating distribution 1 ."}, {"id": 85, "string": "Cross-validation."}, {"id": 86, "string": "To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation."}, {"id": 87, "string": "In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data."}, {"id": 88, "string": "Evaluation metric."}, {"id": 89, "string": "The model outputs are first uniformly re-scaled into [0, 10], mirroring the range of ratings in practice."}, {"id": 90, "string": "Thereafter, akin to (Yannakoudakis et al., 2011; Chen and He, 2013; Alikaniotis et al., 2016) , we report our results primarily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth."}, {"id": 91, "string": "Pearson correlation coefficient (PCC) and Spearman rankorder correlation coefficient (SCC) are also reported."}, {"id": 92, "string": "The correlations obtained from individual folds, as well as the average over all eight folds, are reported as the ultimate results."}, {"id": 93, "string": "Competing models."}, {"id": 94, "string": "Since the promptindependent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction, serving as baselines."}, {"id": 95, "string": "This is due to the facts that the 1 Details of this dataset can be found at https://www."}, {"id": 96, "string": "kaggle.com/c/asap-aes."}, {"id": 97, "string": "No."}, {"id": 98, "string": "Feature 1 Mean & variance of word length in characters 2 Mean & variance of sentence length in words 3 Essay length in characters and words 4 Number of prepositions and commas 5 Number of unique words in an essay 6 Mean number of clauses per sentence 7 Mean length of clauses 8 Maximum number of clauses of a sentence in an essay 9 Number of spelling errors 10 Average depth of the parser tree of each sentence in an essay 11 Average depth of each leaf node in the parser tree of each sentence prompt-dependent and -independent models differ a lot in terms of problem settings and model designs, especially in their requirements for the training data, where the latter ones release the prompt-dependent requirements and thereby are accessible to more data."}, {"id": 99, "string": "-RankSVM, using handcrafted features for AES (Yannakoudakis et al., 2011; Chen et al., 2014) , is trained on a set of pre-defined promptindependent features as listed in Table 2 , where the features are standardized beforehand to remove the mean and variance."}, {"id": 100, "string": "The RankSVM is also used for the prompt-independent stage in our proposed TDNN model."}, {"id": 101, "string": "In particular, the linear kernel RankSVM 2 is employed, where C is set to 5 according to our pilot experiments."}, {"id": 102, "string": "-2L-LSTM."}, {"id": 103, "string": "Two-layer bi-LSTM with GloVe for AES (Alikaniotis et al., 2016) is employed as another baseline."}, {"id": 104, "string": "Regularized word embeddings are dropped to avoid over-fitting the prompt-specific features."}, {"id": 105, "string": "-CNN-LSTM."}, {"id": 106, "string": "This model (Taghipour and Ng, 2016 ) employs a convolutional (CNN) layer over one-hot representations of words, followed by an LSTM layer to encode word sequences in a given essay."}, {"id": 107, "string": "A linear layer with sigmoid activation function is then employed to predict the essay rating."}, {"id": 108, "string": "-CNN-LSTM-ATT."}, {"id": 109, "string": "This model ) employs a CNN layer to encode word sequences into sentences, followed by an LSTM layer to generate the essay representation."}, {"id": 110, "string": "An attention mechanism is added to model the influence of individual sentences on the final essay representation."}, {"id": 111, "string": "For the proposed TDNN model, as introduced in Section 2.2, different variants of TDNN are examined by using one or multiple components out of the semantic, POS and the syntactic networks."}, {"id": 112, "string": "The combinations being considered are listed in the following."}, {"id": 113, "string": "In particular, the dimensions of POS tags and syntactic network are fixed to 50, whereas the sizes of the hidden units in LSTM, as well as the output units of the linear layers are tuned by grid search."}, {"id": 114, "string": "-TDNN(Sem) only includes the semantic building block, which is similar to the two-layer LSTM neural network from (Alikaniotis et al., 2016) but without regularizing the word embeddings; -TDNN(Sem+POS) employs the semantic and the POS building blocks; -TDNN(Sem+Synt) uses the semantic and the syntactic network building blocks; -TDNN(POS+Synt) includes the POS and the syntactic network building blocks; -TDNN(ALL) employs all three building blocks."}, {"id": 115, "string": "The use of POS or syntactic network alone is not presented for brevity given the facts that they perform no better than TDNN(POS+Synt) in our pilot experiments."}, {"id": 116, "string": "Source code of the TDNN model is publicly available to enable further comparison 3 ."}, {"id": 117, "string": "Results and Analyzes In this section, the evaluation results for different competing methods are compared and analyzed in terms of their agreements with the manual ratings using three correlation metrics, namely, QWK, PCC and SCC, where the best results for each prompt is highlighted in bold in Table 3 ."}, {"id": 118, "string": "It can be seen that, for seven out of all eight prompts, the proposed TDNN variants outperform the baselines by a margin in terms of QWK, and the TDNN variant with semantic and syntactic features, namely, TDNN(Sem+Synt), consistently performs the best among different competing methods."}, {"id": 119, "string": "More precisely, as indicated in the bottom right corner in Table 3 , on average, TDNN(Sem+Synt) outperforms the baselines by at least 25.52% under QWK, by 10.28% under PCC, and by 15.66% under SCC, demonstrating that the proposed model not only correlates better with the manual ratings in terms of QWK, but also linearly (PCC) and monotonically (SCC) correlates better with the manual ratings."}, {"id": 120, "string": "As for the four baselines, note that, the relatively underperformed deep models suffer from larger variances of performance under different prompts, e.g., for prompts two and eight, 2L-LSTM's QWK is lower than 0.3."}, {"id": 121, "string": "This actually confirms our choice of RankSVM for the first stage in TDNN, since a more complicated model (like 2L-LSTM) may end up with learning prompt-dependent signals, making it unsuitable for the prompt-independent rating prediction."}, {"id": 122, "string": "As a comparison, RankSVM performs more stable among different prompts."}, {"id": 123, "string": "As for the different TDNN variants, it turns out that the joint uses of syntactic network with semantic or POS features can lead to better performances."}, {"id": 124, "string": "This indicates that, when learning the prompt-dependent signals, apart from the widelyused semantic features, POS features and the sentence structure taggings (syntactic network) are also essential in learning the structure and the arrangement of an essay in response to a particular prompt, thereby being able to improve the results."}, {"id": 125, "string": "It is also worth mentioning, however, when using all three features, the TDNN actually performs worse than when only using (any) two features."}, {"id": 126, "string": "One possible explanation is that the uses of all three features result in a more complicated model, which over-fits the training data."}, {"id": 127, "string": "In addition, recall that the prompt-independent RankSVM model from the first stage enables the proposed TDNN in learning prompt-dependent information without manual ratings for the target prompt."}, {"id": 128, "string": "Therefore, one would like to understand how good the trained RankSVM is in feeding training data for the model in the second stage."}, {"id": 129, "string": "In particular, the precision, recall and F-score (P/R/F) of the essays selected by RanknSVM, namely, the negative ones rated between [0, 4], and the positive ones rated between [8, 10] , are displayed in Figure 4 ."}, {"id": 130, "string": "It can be seen that the P/R/F scores of both positive and negative classes differ a lot among different prompts."}, {"id": 131, "string": "Moreover, it turns out that the P/R/F scores do not necessarily correlate with the performance of the TDNN model."}, {"id": 132, "string": "Take TDNN(Sem+Synt), the best TDNN variant, as an example: as indicated in Table 4 , the performance and the P/R/F scores of the pseudo examples are only weakly correlated in most cases."}, {"id": 133, "string": "To gain a better understanding in how the quality of pseudo examples affects the performance of TDNN, the sanctity of the selected essays are examined."}, {"id": 134, "string": "In Figure 5 , the relative precision of 7616 .7492 .7366 .7993 .7960 .6752 .6903 .7434 TDNN(POS+Synt) .7561 .7591 .7440 .7332 .7983 .7866 .6593 .6759 .7354 TDNN(All) ."}, {"id": 135, "string": "7527 .7609 .7251 .7302 .7974 .7794 .6557 .6874 .7350 Method Prompt 7 Prompt 8 Average RankSVM ."}, {"id": 136, "string": "5858 .6436 .6429 .4075 .5889 .6087 .5462 .6072 .5976 2L-LSTM .6690 .7637 .7607 .2486 .5137 .4979 .4687 .6548 .6214 CNN-LSTM .6609 .6849 .6865 .3812 .4666 .3872 .5362 .6569 .6139 CNN-LSTM-ATT .6002 .6314 .6223 .4468 .5358 .4536 .5057 .6535 .6368 TDNN(Sem) ."}, {"id": 137, "string": "5482 .6957 .6902 .5003 .6083 .6545 .5875 .6779 .6795 TDNN(Sem+POS) .6239 .7111 .7243 .5519 .6219 .6614 .6582 .7103 .7130 TDNN(Sem+Synt) .6587 .7201 .7380 .5741 .6324 .6713 .6856 .7244 .7365 TDNN(POS+Synt) .6464 .7172 .7349 .5631 .6281 .6698 .6784 .7189 .7322 TDNN(All) ."}, {"id": 138, "string": "6396 .7114 .7300 .5622 .6267 .6631 .6682 .7176 .7258 the selected positive and negative training data by RankSVM are displayed for all eight prompts in terms of their concordance with the manual ratings, by computing the number of positive (negative) essays that are better (worse) than all negative (positive) essays."}, {"id": 139, "string": "It can be seen that, such relative precision is at least 80% and mostly beyond 90% on different prompts, indicating that the overlap of the selected positive and negative essays are fairly small, guaranteeing that the deep model in the second stage at least learns from correct labels, which are crucial for the success of our TDNN model."}, {"id": 140, "string": "Beyond that, we further investigate the class balance of the selected training data from the first  stage, which could also influence the ultimate results."}, {"id": 141, "string": "The number of selected positive and negative essays are reported in Table 5 , where for prompts three and eight the training data suffers from serious imbalanced problem, which may explain their lower performance (namely, the two lowest QWKs among different prompts)."}, {"id": 142, "string": "On one hand, this is actually determined by real distribution of ratings for a particular prompt, e.g., how many essays are with an extreme quality for a given prompt in the target data."}, {"id": 143, "string": "On the other hand, a fine-grained tuning of the RankSVM (e.g., tuning C + and C \u2212 for positive and negative exam-ples separately) may partially resolve the problem, which is left for the future work."}, {"id": 144, "string": "Related Work Classical regression and classification algorithms are widely used for learning the rating model based on a variety of text features including lexical, syntactic, discourse and semantic features (Larkey, 1998; Rudner, 2002; Attali and Burstein, 2006; Mcnamara et al., 2015; Phandi et al., 2015) ."}, {"id": 145, "string": "There are also approaches that see AES as a preference ranking problem by applying learning to ranking algorithms to learn the rating model."}, {"id": 146, "string": "Results show improvement of learning to rank approaches over classical regression and classification algorithms (Chen et al., 2014; Yannakoudakis et al., 2011) ."}, {"id": 147, "string": "In addition, Chen & He propose to incorporate the evaluation metric into the loss function of listwise learning to rank for AES (Chen and He, 2013) ."}, {"id": 148, "string": "Recently, there have been efforts in developing AES approaches based on deep neural networks (DNN), for which feature engineering is not required."}, {"id": 149, "string": "Taghipour & Ng explore a variety of neural network model architectures based on recurrent neural networks which can effectively encode the information required for essay scoring and learn the complex connections in the data through the non-linear neural layers (Taghipour and Ng, 2016) ."}, {"id": 150, "string": "Alikaniotis et al."}, {"id": 151, "string": "introduce a neural network model to learn the extent to which specific words contribute to the text's score, which is embedded in the word representations."}, {"id": 152, "string": "Then a two-layer bi-directional Long-Short Term Memory networks (bi-LSTM) is used to learn the meaning of texts, and finally the essay score is predicted through a mutli-layer feed-forward network (Alikaniotis et al., 2016) ."}, {"id": 153, "string": "Dong & Zhang employ a hierarchical convolutional neural network (CN-N) model, with a lower layer representing sentence structure and an upper layer representing essay structure based on sentence representations, to learn features automatically (Dong and Zhang, 2016) ."}, {"id": 154, "string": "This model is later improved by employing attention layers."}, {"id": 155, "string": "Specifically, the model learns text representation with LSTMs which can model the coherence and co-reference among sequences of words and sentences, and uses attention pooling to capture more relevant words and sentences that contribute to the final quality of essays ."}, {"id": 156, "string": "Song et al."}, {"id": 157, "string": "propose a deep model for identifying discourse modes in an essay (Song et al., 2017) ."}, {"id": 158, "string": "While the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a promptindependent setting remains to be explored."}, {"id": 159, "string": "Chen & He studied the usefulness of promptindependent text features and achieved a humanmachine rating agreement slightly lower than the use of all text features (Chen and He, 2013) for prompt-dependent essay scoring prediction."}, {"id": 160, "string": "A constrained multi-task pairwise preference learning approach was proposed in (Cummins et al., 2016) to combine essays from multiple prompts for training."}, {"id": 161, "string": "However, as shown by (Dong and Zhang, 2016; Zesch et al., 2015; Phandi et al., 2015) , straightforward applications of existing AES methods for prompt-independent AES lead to a poor performance."}, {"id": 162, "string": "Conclusions & Future Work This study aims at addressing the promptindependent automated essay scoring (AES), where no rated essay for the target prompt is available."}, {"id": 163, "string": "As demonstrated in the experiments, two kinds of established prompt-dependent AES models, namely, RankSVM for AES (Yannakoudakis et al., 2011; Chen et al., 2014) and the deep models for AES (Alikaniotis et al., 2016; Taghipour and Ng, 2016; , fail to provide satisfactory performances, justifying our arguments in Section 1 that the application of estab-lished prompt-dependent AES models on promptindependent AES is not straightforward."}, {"id": 164, "string": "Therefore, a two-stage TDNN learning framework was proposed to utilize the prompt-independent features to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consuming semantic, part-of-speech, and syntactic signals."}, {"id": 165, "string": "Through the experiments on the ASAP dataset, the proposed TDNN model outperforms the baselines, and leads to promising improvement in the human-machine agreement."}, {"id": 166, "string": "Given that our approach in this paper is similar to the methods for transductive transfer learning (Pan and Yang, 2010), we argue that the proposed TDNN could be further improved by migrating the non-target training data to the target prompt (Busto and Gall, 2017) ."}, {"id": 167, "string": "Further study of the uses of transfer learning algorithms on promptindependent AES needs to be undertaken."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 18}, {"section": "Two-stage Deep Neural Network for AES", "n": "2", "start": 19, "end": 27}, {"section": "Overview", "n": "2.1", "start": 28, "end": 34}, {"section": "Building Blocks", "n": "2.2", "start": 35, "end": 69}, {"section": "Objective and Training", "n": "2.3", "start": 70, "end": 116}, {"section": "Results and Analyzes", "n": "4", "start": 117, "end": 143}, {"section": "Related Work", "n": "5", "start": 144, "end": 161}, {"section": "Conclusions & Future Work", "n": "6", "start": 162, "end": 167}], "figures": [{"filename": "../figure/image/966-Figure1-1.png", "caption": "Figure 1: The architecture of the TDNN framework for prompt-independent AES.", "page": 1, "bbox": {"x1": 313.92, "x2": 519.36, "y1": 176.64, "y2": 294.24}}, {"filename": "../figure/image/966-Table4-1.png", "caption": "Table 4: Linear correlations between the performance of TDNN(Sem+Synt) and the precision, recall, and F-score of the selected pseudo examples.", "page": 6, "bbox": {"x1": 72.0, "x2": 286.08, "y1": 565.4399999999999, "y2": 643.1999999999999}}, {"filename": "../figure/image/966-Table3-1.png", "caption": "Table 3: Correlations between AES and manual ratings for different competing methods are reported for individual prompts. The average results among different prompts are summarized in the bottom right. The best results are highlighted in bold for individual prompts.", "page": 6, "bbox": {"x1": 72.0, "x2": 503.03999999999996, "y1": 62.879999999999995, "y2": 498.24}}, {"filename": "../figure/image/966-Table5-1.png", "caption": "Table 5: The numbers of the selected positive and negative essays for each prompt.", "page": 6, "bbox": {"x1": 73.92, "x2": 288.0, "y1": 692.64, "y2": 732.0}}, {"filename": "../figure/image/966-Figure4-1.png", "caption": "Figure 4: The precision, recall and F-score of the pseudo negative or positive examples, which are rated within [0, 4] or [8, 10] by RankSVM.", "page": 7, "bbox": {"x1": 56.64, "x2": 519.36, "y1": 66.72, "y2": 262.56}}, {"filename": "../figure/image/966-Figure5-1.png", "caption": "Figure 5: The sanctity of the selected positive and negative essays by RankSVM. The x-axis indicates different prompts and the y-axis is the relative precision.", "page": 7, "bbox": {"x1": 45.6, "x2": 310.08, "y1": 316.8, "y2": 516.0}}, {"filename": "../figure/image/966-Figure3-1.png", "caption": "Figure 3: An example of the context-free phrase structure grammar tree.", "page": 3, "bbox": {"x1": 72.0, "x2": 292.32, "y1": 580.8, "y2": 724.3199999999999}}, {"filename": "../figure/image/966-Figure2-1.png", "caption": "Figure 2: The model architecture of the proposed hybrid deep learning model.", "page": 3, "bbox": {"x1": 72.0, "x2": 526.0799999999999, "y1": 61.44, "y2": 276.0}}, {"filename": "../figure/image/966-Table1-1.png", "caption": "Table 1: Statistics for the ASAP dataset.", "page": 4, "bbox": {"x1": 84.96, "x2": 277.44, "y1": 62.4, "y2": 165.12}}, {"filename": "../figure/image/966-Table2-1.png", "caption": "Table 2: Handcrafted features used in learning the prompt-independent RankSVM.", "page": 4, "bbox": {"x1": 306.71999999999997, "x2": 515.04, "y1": 62.4, "y2": 225.12}}]}