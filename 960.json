{"title": "Multi-Label Transfer Learning for Multi-Relational Semantic Similarity", "abstract": "Multi-relational semantic similarity datasets define the semantic relations between two short texts in multiple ways, e.g., similarity, relatedness, and so on. Yet, all the systems to date designed to capture such relations target one relation at a time. We propose a multi-label transfer learning approach based on LSTM to make predictions for several relations simultaneously and aggregate the losses to update the parameters. This multi-label regression approach jointly learns the information provided by the multiple relations, rather than treating them as separate tasks. Not only does this approach outperform the single-task approach and the traditional multi-task learning approach, it also achieves state-of-the-art performance on all but one relation of the Human Activity Phrase dataset.", "text": [{"id": 0, "string": "Introduction Semantic similarity, or relating short texts or sentences 1 in a semantic space -be those phrases, sentences or short paragraphs -is a task that requires systems to determine the degree of equivalence between the underlying semantics of the two sentences."}, {"id": 1, "string": "Although relatively easy for humans, this task remains one of the most difficult natural language understanding problems."}, {"id": 2, "string": "The task has been receiving significant interest from the research community."}, {"id": 3, "string": "For instance, from 2012 to 2017, the International Workshop on Semantic Evaluation (SemEval) has been holding the Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012 (Agirre et al., , 2013b (Agirre et al., , 2015 (Agirre et al., , 2016 Cer et al., 2017) , dedicated to tackling this problem, with close to 100 team submissions each year."}, {"id": 4, "string": "In some semantic similarity datasets, an example consists of a sentence pair and a single annotated similarity score, while in others, each pair 1 In this work, we do not consider word level similarity."}, {"id": 5, "string": "comes with multiple annotations."}, {"id": 6, "string": "We refer to the latter as multi-relational semantic similarity tasks."}, {"id": 7, "string": "The inclusion of multiple annotations per example is motivated by the fact that there can be different relations, namely different types of similarity between two sentences."}, {"id": 8, "string": "So far, these relations have been treated as separate tasks, where a model trains and tests on one relation at a time while ignoring the rest."}, {"id": 9, "string": "However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information."}, {"id": 10, "string": "Thus, training jointly on multiple relations may improve performance on one or more relations."}, {"id": 11, "string": "We propose a joint multi-label transfer learning setting based on LSTM, and show that it can be an effective solution for the multi-relational semantic similarity tasks."}, {"id": 12, "string": "Due to the small size of multirelational semantic similarity datasets and the recent success of LSTM-based sentence representations (Wieting and Gimpel, 2018; Conneau et al., 2017) , the model is pre-trained on a large corpus and transfer learning is applied using fine-tuning."}, {"id": 13, "string": "In our setting, the network is jointly trained on multiple relations by outputting multiple predictions (one for each relation) and aggregating the losses during back-propagation."}, {"id": 14, "string": "This is different from the traditional multi-task learning setting where the model makes one prediction at a time, switching between the tasks."}, {"id": 15, "string": "We treat the multi-task setting and the single-task setting (i.e., where a separate model is learned for each relation) as baselines, and show that the multi-label setting outperforms them in many cases, achieving state-of-the-art performance on all but one relation of the Human Activity Phrase dataset (Wilson and Mihalcea, 2017 )."}, {"id": 16, "string": "In addition to success on multi-relational semantic similarity tasks, the multi-label transfer learning setting that we propose can easily be paired with other neural network architectures and applied to any dataset with multiple annotations available for each training instance."}, {"id": 17, "string": "Multi-Label Transfer Learning We introduce a multi-label transfer learning setting by modifying the architecture of the LSTMbased sentence encoder, specifically designed for multi-relational semantic similarity tasks."}, {"id": 18, "string": "Architecture We employ the \"hard-parameter sharing\" setting (Caruana, 1998) , where some hidden layers are shared across multiple tasks while each task has its own specific output layer."}, {"id": 19, "string": "As shown in Figure 1 , using an example of a semantic similarity dataset with two relations, sentence L and sentence R in a pair are first mapped to word vector sequences and then encoded as sentence embeddings."}, {"id": 20, "string": "Up to this step, the choice of the word embedding matrix and sentence encoder is flexible, and we outline our choice in the sections to follow."}, {"id": 21, "string": "For each relation that has been annotated with a ground-truth label, a dedicated output dense layer takes the two sentence embeddings as input and outputs a probability distribution across the range of possible scores."}, {"id": 22, "string": "The output dense layers follow the methods of Tai et al."}, {"id": 23, "string": "(2015) ."}, {"id": 24, "string": "With two such dense output layers, two losses are calculated, one for each relation."}, {"id": 25, "string": "The total loss is calculated as the sum of the two losses for backpropagation which updates all parameters in the end-to-end network."}, {"id": 26, "string": "Model We use InferSent (Conneau et al., 2017) as the sentence encoder due to its outstanding performances reported on various semantic similarity tasks."}, {"id": 27, "string": "Due to the small sizes of the evaluation datasets, we use the sentence encoder pre-trained on the Stanford Natural Language Inference corpus (Bowman et al., 2015) and Multi-Genre Natural Language Inference corpus (Williams et al., 2018) , and transfer to the semantic similarity tasks using fine-tuning."}, {"id": 28, "string": "In this process, the output layers for multi-label learning discussed above are stacked on top of the InferSent network, forming an end-to-end model for training and testing on semantic similarity tasks."}, {"id": 29, "string": "Comparison with Multi-Task Learning Neither multi-task nor multi-label learning have been used for multi-relational semantic similarity datasets."}, {"id": 30, "string": "For these datasets, either multi-task or multi-label learning can be achieved by treating each relation as a \"task.\""}, {"id": 31, "string": "The key differences between the two are the relations involved in each forward-backward pass and the timing of the parameter updates."}, {"id": 32, "string": "Consider a training step in the two-relation example in Figure 1 : A multi-task learning model would pick a batch of sentences pairs, only consider Label L, only calculate Loss L, and all parameters except those of dense layer d R are updated."}, {"id": 33, "string": "Then, within the same batch, 2 the model would only consider Label R, only calculate Loss R, and all parameters except those of dense layer d L are updated."}, {"id": 34, "string": "A multi-label learning model (our model) would pick a batch of sentences pairs, consider both Label L and Label R, calculate Loss L and Loss R, aggregate them as the total loss, and update all parameters."}, {"id": 35, "string": "Experiments To show the effectiveness of the multi-label transfer learning setting, we experiment on three semantic similarity datasets with multiple relations annotated, and use one LSTM-based sentence encoder that has been very successful in many downstream tasks."}, {"id": 36, "string": "Datasets We study three semantic similarity datasets with multiple relations with texts of different lengths, spanning phrases, sentences, and short paragraphs."}, {"id": 37, "string": "Human Activity Phrase (Wilson and Mihalcea, 2017) : a collection of pairs of phrases regarding human activities, annotated with the following four different relations."}, {"id": 38, "string": "\u2022 Similarity (SIM): The degree to which the two activity phrases describe the same thing, semantic similarity in a strict sense."}, {"id": 39, "string": "Example of high similarity phrases: to watch a film and to see a movie."}, {"id": 40, "string": "\u2022 Relatedness (REL): The degree to which the activities are related to one another, a general semantic association between two phrases."}, {"id": 41, "string": "Example of strongly related phrases: to give a gift and to receive a present."}, {"id": 42, "string": "\u2022 Motivational alignment (MA): The degree to which the activities are (typically) done with similar motivations."}, {"id": 43, "string": "Example of phrases with potentially similar motivations: to eat dinner with family members and to visit relatives."}, {"id": 44, "string": "\u2022 Perceived actor congruence (PAC): The degree to which the activities are expected to be done by the same type of person."}, {"id": 45, "string": "An example of a pair with a high PAC score: to pack a suitcase and to travel to another state."}, {"id": 46, "string": "The phrases are generated, paired and scored on Amazon Mechanical Turk."}, {"id": 47, "string": "3 The annotated input."}, {"id": 48, "string": "3 https://www.mturk.com/ scores range from 0 to 4 for SIM, REL and MA, and \u22122 to 2 for PAC."}, {"id": 49, "string": "The evaluation is based on the Spearman's \u03c1 correlation coefficient between the systems' predicted scores and the human annotations."}, {"id": 50, "string": "There are 1,000 pairs in the dataset."}, {"id": 51, "string": "We also use the supplemental 1,373 pairs from Zhang et al."}, {"id": 52, "string": "(2018) in which 1,000 pairs are randomly selected for training and the rest are used for development."}, {"id": 53, "string": "We then treat the original 1,000 pairs as a held-out test set so that our results are directly comparable with those previously reported."}, {"id": 54, "string": "SICK (Marelli et al., 2014b,a) : the Sentences Involving Compositional Knowledge benchmark, which includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena."}, {"id": 55, "string": "Each pair of sentences is annotated in two dimensions: relatedness and entailment."}, {"id": 56, "string": "The relatedness score ranges from 1 to 5, and Pearson's r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral."}, {"id": 57, "string": "There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split."}, {"id": 58, "string": "The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm."}, {"id": 59, "string": "Due to the lack of human supervision in the process, some sentence pairs display minimal difference in semantic components, making the SICK tasks simpler than the others we study."}, {"id": 60, "string": "Typed-Similarity (Agirre et al., 2013b): a collection of meta-data describing books, paintings, films, museum objects and archival records taken from Europeana, 4 presented as the pilot track in the SemEval 2013 STS shared task."}, {"id": 61, "string": "Typically, the items consist of title, subject, description, and so on, describing a cultural heritage item and, sometimes, a thumbnail of the item itself."}, {"id": 62, "string": "For the purpose of measuring semantic similarity, we concatenate all the textual entries such as title, creator, subject and description into a short paragraph that is used as input, although the annotations might be informed of the image aspects of the meta-data."}, {"id": 63, "string": "Each pair of items is annotated on eight dimensions of similarity: general similarity, author, people involved, time, location, event or action involved, subject and description."}, {"id": 64, "string": "There are 750 pairs in the train split, of which we randomly sample 500 for training and 250 for development, and 721 in the test split."}, {"id": 65, "string": "Pearson's r is used for evaluation."}, {"id": 66, "string": "Baselines We compare the multi-label setting with two baselines: \u2022 Single-task, where each relation is treated as an individual task."}, {"id": 67, "string": "For each relation, a model with only one output dense layer is trained and tested, ignoring the annotations of all other relations."}, {"id": 68, "string": "\u2022 Multi-task, where only one relation is involved during each round of feed-forward and back-propagation."}, {"id": 69, "string": "Experimental Details In each experiment, we use stochastic gradient descent and a batch size of 16."}, {"id": 70, "string": "We tune the learning rate over {0.1, 0.5, 1, 5} and number of epochs over {10, 20}."}, {"id": 71, "string": "For each dataset discussed above, we tune these hyperparameters on the development set."}, {"id": 72, "string": "All other hyperparameters maintain their values from the original code."}, {"id": 73, "string": "5 In the single-task setting, the model is trained and tested on each relation, ignoring the annotations of other relations."}, {"id": 74, "string": "In the multi-task settings, the model is trained and tested on all the relations in a dataset."}, {"id": 75, "string": "In the multitask setting, relations are presented to the model in the order they are listed in the result tables within each batch."}, {"id": 76, "string": "Evaluation The results are shown in Tables 1, 2 and 3."}, {"id": 77, "string": "For every experiment (represented by a cell in the tables), 30 runs with different random seeds are recorded and the average is reported."}, {"id": 78, "string": "For each relation (each column in the tables), let the true mean performance of multi-label learning, singletask baseline and multi-task baseline be \u00b5 MLL , \u00b5 single , \u00b5 MTL , respectively."}, {"id": 79, "string": "Two one-sided Student's t-tests are conducted to test if multi-label learning outperforms the baselines for that relation."}, {"id": 80, "string": "The significance level is chosen to be 0.05."}, {"id": 81, "string": "A down-arrow \u2193 indicates that our proposed multilabel learning underperforms a baseline, while an up-arrow \u2191 indicates that our proposed multi-label learning outperforms a baseline."}, {"id": 82, "string": "5 https://github.com/facebookresearch/InferSent 5 Discussion Results For the Human Activity Phrase dataset, the singletask setting already achieves state-of-the-art performances on SIM, REL and PAC relations, surpassing the previous best results reported by Zhang et al."}, {"id": 83, "string": "(2018) , which achieved Spearman's correlation coefficient of .710 in SIM, .715 in REL, .690 in MA and .549 in PAC."}, {"id": 84, "string": "This approach is based on fine-tuning a bi-directional LSTM with average-pooling pre-trained on translated texts (Wieting and Gimpel, 2018) ."}, {"id": 85, "string": "Using multi-label learning, our model is able to gain a statistically significant improvement in the performance of REL compared to the single-task setting, while maintaining performance for the other relations."}, {"id": 86, "string": "The traditional multi-task setting, however, performs significantly worse than the other settings."}, {"id": 87, "string": "For the entailment task on the SICK dataset, our multi-label setting outperforms the singletask baseline and the previous best results of In-ferSent."}, {"id": 88, "string": "These best results consisted of an accuracy of 86.3% achieved using a logistic regression classifier and sentence embeddings generated by pre-trained InferSent as features (Conneau et al., 2017 )."}, {"id": 89, "string": "In the relatedness task, this setting achieved a Pearson's correlation coefficient of .885, which even our our multi-label setting is unable to beat."}, {"id": 90, "string": "However, the multi-label setting does have a statistically significant performance gain compared to the single-task setting in the relatedness task, while the traditional multi-task setting underperforms the other settings."}, {"id": 91, "string": "For the Typed-Similarity dataset, the previous best results are achieved using rich feature engineering without the use of sentence embeddings, with a different scoring scheme for each relation (Agirre et al., 2013a) ."}, {"id": 92, "string": "While this method yielded better results than all of the transfer learning approaches we compare, it should be noted that this approach is specific to tackling this dataset, unlike the transfer learning settings that are generalizable to other scenarios."}, {"id": 93, "string": "One potential reason for the discrepancy in performance is that some relations such as time, people involved, or events may be easily or sometimes trivially captured by information retrieval techniques such as named entity recognition."}, {"id": 94, "string": "Using sentence embeddings and transfer learning for all the relations, though simpler, may face greater challenge in the rela-   tions mentioned above."}, {"id": 95, "string": "Among the three transfer learning approaches, our multi-label setting is still superior, outperforming the single-task setting in over half of the relations, and outperforming the multi-task setting in all relations."}, {"id": 96, "string": "Empirical Recommendation While our results above show that multi-label learning is almost always the most effective way to transfer sentence embeddings in multi-relational semantic similarity tasks, in some situations simply training with one relation might yield better performance (such as the general similarity relation in the Typed-Similarity dataset)."}, {"id": 97, "string": "This suggests that the choice of multi-label learning or single-task learning can be tuned as a hyperparameter empirically for the optimal performance on a task."}, {"id": 98, "string": "Other Considerations and Discussions In the multi-label setting, we calculate the total loss by summing the loss from each dimension."}, {"id": 99, "string": "We also explore weighting the loss from each di-mension by factors of 2, 5 and 10, but doing so hurts the performance for all dimensions."}, {"id": 100, "string": "In the multi-task setting, we attempt different ordering of the dimensions when presenting them to the model within a batch of examples, but the difference in performance is not statistically significant."}, {"id": 101, "string": "Furthermore, the multi-task setting takes about n times longer to train than the multi-label setting, where n is number of dimensions of annotations."}, {"id": 102, "string": "Conclusions We introduced a multi-label transfer learning setting designed specifically for semantic similarity tasks with multiple relations annotations."}, {"id": 103, "string": "By experimenting with a variety of relations in three datasets, we showed that the multi-label setting can outperform single-task and traditional multitask settings in many cases."}, {"id": 104, "string": "Future work includes exploring the performance of this setting with other sentence encoders, as well as multi-label datasets outside of the domain of semantic similarity."}, {"id": 105, "string": "This may include NLP datasets annotated with author information for multiple dimensions, or computer vision datasets with multiple annotations for scenes."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 16}, {"section": "Multi-Label Transfer Learning", "n": "2", "start": 17, "end": 17}, {"section": "Architecture", "n": "2.1", "start": 18, "end": 25}, {"section": "Model", "n": "2.2", "start": 26, "end": 28}, {"section": "Comparison with Multi-Task Learning", "n": "2.3", "start": 29, "end": 34}, {"section": "Experiments", "n": "3", "start": 35, "end": 35}, {"section": "Datasets", "n": "3.1", "start": 36, "end": 65}, {"section": "Baselines", "n": "3.2", "start": 66, "end": 68}, {"section": "Experimental Details", "n": "3.3", "start": 69, "end": 75}, {"section": "Evaluation", "n": "4", "start": 76, "end": 81}, {"section": "Results", "n": "5.1", "start": 82, "end": 95}, {"section": "Empirical Recommendation", "n": "5.2", "start": 96, "end": 97}, {"section": "Other Considerations and Discussions", "n": "5.3", "start": 98, "end": 101}, {"section": "Conclusions", "n": "6", "start": 102, "end": 105}], "figures": [{"filename": "../figure/image/960-Table2-1.png", "caption": "Table 2: The performance in Spearman\u2019s \u03c1 on the Human Activity Phrase dataset.", "page": 4, "bbox": {"x1": 86.88, "x2": 275.03999999999996, "y1": 200.64, "y2": 257.28}}, {"filename": "../figure/image/960-Table3-1.png", "caption": "Table 3: The performance in Pearson\u2019s r on the SICK dataset, in accordance with the specification of the dataset to allow for direct comparison with previous results.", "page": 4, "bbox": {"x1": 94.56, "x2": 268.32, "y1": 307.68, "y2": 364.32}}, {"filename": "../figure/image/960-Table1-1.png", "caption": "Table 1: The performance in Pearson\u2019s r on the Typed-Similarity dataset, in accordance with the specification of the dataset to allow for direct comparison with previous results. The results of single task and multi-task learning (MTL) are followed by \u2191 if it is statically significantly lower than those of multi-label learning (MLL), and they are followed by \u2193 otherwise.", "page": 4, "bbox": {"x1": 94.56, "x2": 503.03999999999996, "y1": 62.879999999999995, "y2": 119.03999999999999}}, {"filename": "../figure/image/960-Figure1-1.png", "caption": "Figure 1: Overview of the multi-label architecture.", "page": 1, "bbox": {"x1": 72.0, "x2": 286.08, "y1": 61.44, "y2": 490.08}}]}