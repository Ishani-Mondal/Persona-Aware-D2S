{"title": "Obtaining SMT dictionaries for related languages", "abstract": "This study explores methods for developing Machine Translation dictionaries on the basis of word frequency lists coming from comparable corpora. We investigate (1) various methods to measure the similarity of cognates between related languages, (2) detection and removal of noisy cognate translations using SVM ranking. We show preliminary results on several Romance and Slavonic languages.", "text": [{"id": 0, "string": "Introduction Cognates are words having similarities in their spelling and meaning in two languages, either because the two languages are typologically related, e.g., maladie vs malattia ('disease'), or because they were both borrowed from the same source (informatique vs informatica)."}, {"id": 1, "string": "The advantage of their use in Statistical Machine Translation (SMT) is that the procedure can be based on comparable corpora, i.e., similar corpora which are not translations of each other (Sharoff et al., 2013) ."}, {"id": 2, "string": "Given that there are more sources of comparable corpora in comparison to parallel ones, the lexicon obtained from them is likely to be richer and more variable."}, {"id": 3, "string": "Detection of cognates is a well-known task, which has been explored for a range of languages using different methods."}, {"id": 4, "string": "The two main approaches applied to detection of the cognates are the generative and discriminative paradigms."}, {"id": 5, "string": "The first one is based on detection of the edit distance between potential candidate pairs."}, {"id": 6, "string": "The distance can be a simple Levenshtein distance, or a distance measure with the scores learned from an existing parallel set (Tiedemann, 1999; Mann and Yarowsky, 2001) ."}, {"id": 7, "string": "The discriminative paradigm uses standard approaches to machine learning, which are based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009) ."}, {"id": 8, "string": "Given that SMT is usually based on a full-form lexicon, one of the possible issues in generation of cognates concerns the similarity of words in their root form vs the similarity in endings."}, {"id": 9, "string": "For example, the Ukrainian wordform \u00e1\u00eb\u00e8ae\u00ed\u00fc\u00ee\u00e3\u00ee 'near gen ' is cognate to Russian \u00e1\u00eb\u00e8ae\u00ed\u00e5\u00e3\u00ee, the root is identical, while the ending is considerably different (\u00fc\u00ee\u00e3\u00ee vs \u00e5\u00e3\u00ee)."}, {"id": 10, "string": "Regular differences in the endings, which are shared across a large number of words, can be learned separately from the regular differences in the roots."}, {"id": 11, "string": "One also needs to take into account the false friends among cognates."}, {"id": 12, "string": "For example, dise\u00f1ar means 'to design' in Spanish vs desenhar in Portuguese means 'to draw'."}, {"id": 13, "string": "There are also often cases of partial cognates, when the words share the meaning in some contexts, but not in others, e.g., ae\u00e5\u00ed\u00e0 in Russian means 'wife', while its Bulgarian cognate ae\u00e5\u00ed\u00e0 has two meanings: 'wife' and 'woman'."}, {"id": 14, "string": "Yet another complexity concerns a frequency mismatch."}, {"id": 15, "string": "Two cognates might differ in their frequency."}, {"id": 16, "string": "For example, dibujo in Spanish ('a drawing', rank 1779 in the Wikipedia frequency list) corresponds to a relatively rare cognate word debuxo in Portuguese (rank 104,514 in Wikipedia), while another Portuguese word desenho is more commonly used in this sense (rank 884 in the Portuguese Wikipedia)."}, {"id": 17, "string": "For MT tasks we need translations that are equally appropriate in the source and target language, therefore cognates useful for a high-quality dictionary for SMT need to have roughly the same frequency in comparable corpora and they need to be used in similar contexts."}, {"id": 18, "string": "This study investigates the settings for extracting cognates for related languages in Romance and Slavonic language families for the task of reducing the number of unknown words for SMT."}, {"id": 19, "string": "This in-cludes the effects of having constraints for the cognates to be similar in their roots and in the endings, to occur in distributionally similar contexts and to have similar frequency."}, {"id": 20, "string": "Methodology The methodology for producing the list of cognates is based on the following steps: 1) Produce several lists of cognates using a family of distance measures, discussed in Section 2.1 from comparable corpora, 2) Prune the candidate lists by ranking items, this is done using a Machine Learning (ML) algorithm trained over parallel corpora for detecting the outliers, discussed in Section 2.2; The initial frequency lists for alignment are based Wikipedia dumps for the following languages: Romance (French, Italian, Spanish, Portuguese) and Slavonic (Bulgarian, Russian, Ukrainian), where the target languages are Spanish and Russian 1 ."}, {"id": 21, "string": "Cognate detection We extract possible lists of cognates from comparable corpora by using a family of similarity measures: L direct matching between the languages using Levenshtein distance (Levenshtein, 1966) ; L(w s , w t ) = 1 \u2212 ed(w s , w t ) L-R Levenshtein distance with weights computed separately for the roots and for the endings; LR(r s , r t , e s , e t ) = \u03b1\u00d7ed(rs,rt)+\u03b2\u00d7ed(es,et) \u03b1+\u03b2 L-C Levenshtein distance over word with similar number of starting characters (i.e."}, {"id": 22, "string": "prefix); LC(c s , c t ) = 1 \u2212 ed(c s , c t ), same prefix 0, otherwise where ed(., .)"}, {"id": 23, "string": "is the normalised Levenshtein distance in characters between the source word w s and the target word w t ."}, {"id": 24, "string": "The r s and r t are the stems produced by the Snowball stemmer 2 ."}, {"id": 25, "string": "Since the Snowball stemmer does not support Ukrainian and Bulgarian, we used the Russian model for making the stem/ending split."}, {"id": 26, "string": "e s , e t are the characters at the end of a word form given a stem and c s , c t are the first n characters of a word."}, {"id": 27, "string": "In this work, we set the weights \u03b1 = 0.6 and \u03b2 = 0.4 giving more importance to the roots."}, {"id": 28, "string": "We set a higher weight to roots on the L-R, which is language dependent, and compare to the L-C metric, which is language independent."}, {"id": 29, "string": "We transform the Levenshtein distances into similarity metrics by subtracting the normalised distance score from one."}, {"id": 30, "string": "The produced lists contain for each source word the possible n-best target words accordingly to the maximum scores with one of the previous measures."}, {"id": 31, "string": "The n-best list allows possible cognate translations to a given source word that share a part of the surface form."}, {"id": 32, "string": "Different from (Mann and Yarowsky, 2001) , we produce n-best cognate lists scored by edit distance instead of 1-best."}, {"id": 33, "string": "An important problem when comparing comparable corpora is the way of representing the search space, where an exhaustive method compares all the combinations of source and target words (Mann and Yarowsky, 2001) ."}, {"id": 34, "string": "We constraint the search space by comparing each source word against the target words that belong to a frequency window around the frequency of the source word."}, {"id": 35, "string": "This constraint only applies for the L and L-R metrics."}, {"id": 36, "string": "We use Wikipedia dumps for the source and target side processed in the form frequency lists."}, {"id": 37, "string": "We order the target side list into bins of similar frequency and for the source side we filter words that appear only once."}, {"id": 38, "string": "We use the window approach given that the frequency between the corpora under study can not be directly comparable."}, {"id": 39, "string": "During testing we use a wide window of \u00b1200 bins to minimise the loss of good candidate translations."}, {"id": 40, "string": "The second search space constraint heuristic is the L-C metric."}, {"id": 41, "string": "This metric only compares source words with the target words upto a given n prefix."}, {"id": 42, "string": "For c s , c t in L-C , we use the first four characters to compare groups of words as suggested in (Kondrak et al., 2003) ."}, {"id": 43, "string": "Cognate Ranking Given that the n-best lists contain noise, we aim to prune them by an ML ranking model."}, {"id": 44, "string": "However, there is a lack of resources to train a classification model for cognates (i.e."}, {"id": 45, "string": "cognate vs. false friend), as mentioned in (Fi\u0161er and Ljube\u0161i\u0107, 2013) ."}, {"id": 46, "string": "Available data that can be used to judge the cognate lists are the alignment pairs extracted from parallel data."}, {"id": 47, "string": "We decide to use a ranking model to avoid data imbalance present in classification and to use the probability scores of the alignment pairs as ranks, as opposed to the classification model used by (Irvine and Callison-Burch, 2013) ."}, {"id": 48, "string": "Moreover, we also use a popular domain adaptation technique (Daum\u00e9 et al., 2010) given that we have access to different domains of parallel training data that might be compatible with our comparable corpora."}, {"id": 49, "string": "The training data are the alignments between pairs of words where we rank them accordingly to their correspondent alignment probability from the output of GIZA++ (Och and Ney, 2003) ."}, {"id": 50, "string": "We then use a heuristic to prune training data in order to simulate cognate words."}, {"id": 51, "string": "Pairs of words scored below the Levenshtein similarity threshold of 0.5 are not considered as cognates given that they are likely to have a different surface form."}, {"id": 52, "string": "We represent the training and test data with features extracted from different edit distance scores and distributional measures."}, {"id": 53, "string": "The edit distances features are as follows: 1) Similarity measure L and 2) Number of times of each edit operation."}, {"id": 54, "string": "Thus, the model assigns a different importance to each operation."}, {"id": 55, "string": "The distributional feature is based on the cosine between the distributional vectors of a window of n words around the word currently under comparison."}, {"id": 56, "string": "We train distributional similarity models with word2vec (Mikolov et al., 2013a) for the source and target side separately."}, {"id": 57, "string": "We extract the continuous vector for each word in the window, concatenate it and then compute the cosine between the concatenated vectors of the source and the target."}, {"id": 58, "string": "We suspect that the vectors will have similar behaviour between the source and the target given that they are trained under parallel Wikipedia articles."}, {"id": 59, "string": "We develop two ML models: 1) Edit distance scores and 2) Edit distance scores and distributional similarity score."}, {"id": 60, "string": "We use SVMlight (Joachims, 1998) Results and Discussion In this section we describe the data used to produce the n-best lists and train the cognate ranking models."}, {"id": 61, "string": "We evaluate the ranking models with heldout data from each training domain."}, {"id": 62, "string": "We also provide manual evaluation over the ranked n-best lists for error analysis."}, {"id": 63, "string": "Data The n-best lists to detect cognates were extracted from the respective Wikipedias by using the method described in Section 2.1."}, {"id": 64, "string": "The training data for the ranking model consists of different types of parallel corpora."}, {"id": 65, "string": "The parallel corpora are as follows: 1) Wiki-titles we use the inter language links to create a parallel corpus from the tittles of the Wikipedia articles, with about 500K aligned links (i.e."}, {"id": 66, "string": "'sentences') per language pair (about 200k for bg-ru), giving us about 200K training instances per language pair 3 , 2) Opensubs is an open source corpus of subtitles built by the fan community, with 1M sentences, 6M tokens, 100K words, giving about 90K training instances (Tiedemann, 2012) and 3) Zoo is a proprietary corpus of subtitles produced by professional translators, with 100K sentences, 700K tokens, 40K words and giving about 20K training instances per language pair."}, {"id": 67, "string": "Our objective is to create MT dictionaries from the produced n-best lists and we use parallel data as a source of training to prune them."}, {"id": 68, "string": "We are interested in the corpora of subtitles because the chosen domain of our SMT experiments is subtitling, while the proposed ranking method can be used in other application domains as well."}, {"id": 69, "string": "We consider Zoo and Opensubs as two different domains given that they were built by different types of translators and they differ in size and quality."}, {"id": 70, "string": "The heldout data consists of 2K instances for each corpus."}, {"id": 71, "string": "We use Wikipedia documents and Opensusbs subtitles to train word2vec for the distributional similarity feature."}, {"id": 72, "string": "We use the continuous bag-ofwords algorithm for word2vec and set the parameters for training to 200 dimensions and a window of 8 words."}, {"id": 73, "string": "The Wikipedia documents with an average number of 70K documents for each language, and Opensubs subtitles with 1M sentences for each language."}, {"id": 74, "string": "In practice we only use the Wikipedia data given that for Opensubs the model is able to find relatively few vectors, for example a vector is found only for 20% of the words in the pt-es pair."}, {"id": 75, "string": "Evaluation of the Ranking Model We define two ranking models as: model E for edit distance features and model EC for both edit Table 1 shows the results of the ranking procedure."}, {"id": 76, "string": "For the Romance family language pairs the model EC with context features consistently reduces the error compared to the solely use of edit distance metrics."}, {"id": 77, "string": "The only exception is the it-es EC model with poor results for the domain of Wiki-titles."}, {"id": 78, "string": "The models for the Slavonic family behave similarly to the Romance family, where the use of context features reduces the ranking error."}, {"id": 79, "string": "The exception is the bg-ru model on the Opensubs domain."}, {"id": 80, "string": "A possible reason for the poor results on the ites and bg-ru models is that the model often assigns a high similarity score to unrelated words."}, {"id": 81, "string": "For example, in it-es, mortes 'deads' is treated as close to categoria 'category'."}, {"id": 82, "string": "A possible solution is to map the vectors form the source side into the space of the target side via a learned transformation matrix (Mikolov et al., 2013b) ."}, {"id": 83, "string": "Preliminary Results on Comparable Corpora After we extracted the n-best lists for the Romance family comparable corpora, we applied one of the ranking models on these lists and we manually evaluated over a sample of 50 words 4 ."}, {"id": 84, "string": "We set n to 10 for the n-best lists."}, {"id": 85, "string": "We use a frequency window of 200 for the n-best list search heuristic and the domain of the comparable corpora to Wiki-titles 4 The sample consists of words with a frequency between 2K and 5. for the domain adaptation technique."}, {"id": 86, "string": "The purpose of manual evaluation is to decide whether the ML setup is sensible on the objective task."}, {"id": 87, "string": "Each list is evaluated by accuracy at 1 and accuracy at 10."}, {"id": 88, "string": "We also show success and failure examples of the ranking and the n-best lists."}, {"id": 89, "string": "Table 2 shows the preliminary results of the ML model E on a sample of Wikipedia dumps."}, {"id": 90, "string": "The L and L-R lists consistently show poor results."}, {"id": 91, "string": "A possible reason is the amount of errors given the first step to extract the n-best lists."}, {"id": 92, "string": "For example, in pt-es, for the word vivem 'live' the 10-best list only contain one word with a similar meaning viva 'living' but it can be also translated as 'cheers'."}, {"id": 93, "string": "In the pt-es list for the word representa\u00e7\u00e3o 'description' the correct translation representaci\u00f3n is not among the 10-best in the L list."}, {"id": 94, "string": "However, it is present in the 10-best for the L-C list and the ML model EC ranks it in the first place."}, {"id": 95, "string": "The edit distance model E still makes mistakes like with the list L-C, the word vivem 'live' translates into viven 'living' and the correct translation is vivir."}, {"id": 96, "string": "However, given a certain context/sense the previous translation can be correct."}, {"id": 97, "string": "The ranking scores given by the SVM varies from each list version."}, {"id": 98, "string": "For the L-C lists the scores are more uniform in increasing order and with a small variance."}, {"id": 99, "string": "The L and L-R lists show the opposite behaviour."}, {"id": 100, "string": "We add the produced Wikipedia n-best lists with the L metric into a SMT training dataset for the ptes pair."}, {"id": 101, "string": "We use the Moses SMT toolkit (Koehn et al., 2007) to test the augmented datasets."}, {"id": 102, "string": "We compare the augmented model with a baseline both trained by using the Zoo corpus of subtitles."}, {"id": 103, "string": "We use a 1-best list consisting of 100K pairs."}, {"id": 104, "string": "Te dataset used for pt-es baseline is: 80K training sentences, 1K sentences for tuning and 2K sen- Lang Pairs acc@1 acc@10 acc@1 acc@10 acc@1 acc@10  pt-es  20  60  22  59  32  70  it-es  16  53  18  45  44  66  fr-es  10  48  12  51 29 59 A possible reason for low improvement in terms of the BLEU scores is because MT evaluation metrics, such as BLEU, compare the MT output with a human reference."}, {"id": 105, "string": "The human reference translations in our corpus have been done from English (e.g., En\u2192Es), while the test translations come from a related language (En\u2192Pt\u2192Es), often resulting in different paraphrases of the same English source."}, {"id": 106, "string": "While our OOV rate improved, the evaluation scores did not reflected this, because our MT output was still far from the reference even in cases it was otherwise acceptable."}, {"id": 107, "string": "List L List L-R List L-C Conclusions and future Work We have presented work in progress for developing MT dictionaries extracted from comparable resources for related languages."}, {"id": 108, "string": "The extraction heuristic show positive results on the n-best lists that group words with the same starting char-5 https://github.com/clab/fast_align 6 https://kheafield.com/code/kenlm/ 7 The p-value for the uk-ru pair is 0.06 we do not consider this result as statistically significant."}, {"id": 109, "string": "acters, because the used comparable corpora consist of related languages that share a similar orthography."}, {"id": 110, "string": "However, the lists based on the frequency window heuristic show poor results to include the correct translations during the extraction step."}, {"id": 111, "string": "Our ML models based on similarity metrics over parallel corpora show rankings similar to heldout data."}, {"id": 112, "string": "However, we created our training data using simple heuristics that simulate cognate words (i.e."}, {"id": 113, "string": "pairs of words with a small surface difference)."}, {"id": 114, "string": "The ML models are able to rank similar words on the top of the list and they give a reliable score to discriminate wrong translations."}, {"id": 115, "string": "Preliminary results on the addition of the n-best lists into an SMT system show modest improvements compare to the baseline."}, {"id": 116, "string": "However, the OOV rate shows improvements around 10% reduction on word types, because of the wide variety of lexical choices introduced by the MT dictionaries."}, {"id": 117, "string": "Future work involves the addition of unsupervised morphology features for the n-best list extraction, i.e."}, {"id": 118, "string": "first step, given that the use of starting characters shows to be an effective heuristic to prune the search space and language independent."}, {"id": 119, "string": "Finally, we will measure the contribution for all the produced cognate lists, where we can try different strategies to add the dictionaries into an SMT system (Irvine and Callison-Burch, 2014) ."}], "headers": [{"section": "Introduction", "n": "1", "start": 0, "end": 17}, {"section": "Methodology", "n": "2", "start": 18, "end": 20}, {"section": "Cognate detection", "n": "2.1", "start": 21, "end": 42}, {"section": "Cognate Ranking", "n": "2.2", "start": 43, "end": 59}, {"section": "Results and Discussion", "n": "3", "start": 60, "end": 62}, {"section": "Data", "n": "3.1", "start": 63, "end": 74}, {"section": "Evaluation of the Ranking Model", "n": "3.2", "start": 75, "end": 82}, {"section": "Preliminary Results on Comparable Corpora", "n": "3.3", "start": 83, "end": 106}, {"section": "Conclusions and future Work", "n": "4", "start": 107, "end": 119}], "figures": [{"filename": "../figure/image/972-Table2-1.png", "caption": "Table 2: Accuracy at 1 and at 10 results of the ML model E over a sample of 50 words on Wikipedia dumps comparable corpora for the Romance family.", "page": 4, "bbox": {"x1": 132.96, "x2": 464.15999999999997, "y1": 61.44, "y2": 132.96}}, {"filename": "../figure/image/972-Table1-1.png", "caption": "Table 1: Zero/one-error percentage results on heldout test parallel data for each training domain.", "page": 3, "bbox": {"x1": 106.56, "x2": 490.08, "y1": 61.44, "y2": 189.12}}]}